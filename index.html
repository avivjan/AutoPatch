<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Autoâ€‘Patching: Enhancing Multiâ€‘Hop Reasoning in Language Models</title>
  
  <!-- GoogleÂ Fonts -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;700&display=swap" rel="stylesheet" />
  
  <style>
    :root {
      --primary: #2563eb;        /* blueâ€‘600 */
      --primary-dark: #1e4fbe;  /* darker shade */
      --bg: #f8fafc;            /* slateâ€‘50 */
      --text: #111827;          /* slateâ€‘900 */
    }

    * {
      box-sizing: border-box;
      margin: 0;
      padding: 0;
    }

    body {
      font-family: 'Inter', Helvetica, Arial, sans-serif;
      background: var(--bg);
      color: var(--text);
      line-height: 1.6;
    }

    /* â€”â€”â€” Hero â€”â€”â€” */
    .hero {
      padding: 5rem 1.5rem 6rem; /* extra bottom padding for card overlap */
      text-align: center;
      background: linear-gradient(135deg, #6366f1 0%, #3b82f6 100%); /* indigoâ€‘500 â†’ blueâ€‘500 */
      color: #ffffff;
      position: relative;
    }

    .hero h1 {
      font-size: 3rem; /* 48px */
      font-weight: 700;
    }

    .subtitle {
      font-size: 1.25rem; /* 20px */
      margin-top: 0.75rem;
      opacity: 0.9;
      font-weight: 500;
    }

    /* â€”â€”â€” Layout â€”â€”â€” */
    .container {
      max-width: 900px;
      margin: -4rem auto 2.5rem; /* pull card upward */
      padding: 0 1.5rem;
    }

    .card {
      background: #ffffff;
      border-radius: 1rem; /* 16px */
      padding: 2.5rem 2rem;
      box-shadow: 0 10px 25px rgba(0, 0, 0, 0.08);
    }

    /* â€”â€”â€” Metadata â€”â€”â€” */
    .authors {
      font-size: 1.125rem; /* 18px */
      font-weight: 500;
      margin-bottom: 0.25rem;
      color: #374151; /* slateâ€‘700 */
    }

    .institution {
      font-size: 1rem;
      font-style: italic;
      margin-bottom: 2rem;
      color: #6b7280; /* slateâ€‘500 */
    }

    /* â€”â€”â€” Section titles â€”â€”â€” */
    .section-title {
      font-size: 1.5rem; /* 24px */
      font-weight: 600;
      margin: 2.5rem 0 1rem;
      color: var(--primary);
    }

    /* â€”â€”â€” Abstract â€”â€”â€” */
    .abstract {
      text-align: justify;
      font-size: 1.125rem; /* 18px */
    }

    /* â€”â€”â€” Button â€”â€”â€” */
    .btn {
      display: inline-block;
      margin-top: 0.75rem;
      background: var(--primary);
      color: #ffffff;
      padding: 0.75rem 1.5rem;
      border-radius: 0.75rem;
      font-size: 1rem;
      font-weight: 500;
      text-decoration: none;
      transition: background 0.2s ease;
    }

    .btn:hover {
      background: var(--primary-dark);
    }

    /* â€”â€”â€” BibTeX â€”â€”â€” */
    .bibtex {
      background: #f3f4f6; /* slateâ€‘100 */
      border-radius: 0.75rem;
      padding: 1rem 1.25rem;
      font-family: 'Courier New', Courier, monospace;
      font-size: 0.9rem;
      white-space: pre-wrap;
      word-break: break-all;
      color: #374151; /* slateâ€‘700 */
    }

    /* â€”â€”â€” Footer â€”â€”â€” */
    .footer {
      padding: 2rem 0 3rem;
      text-align: center;
      font-size: 0.875rem; /* 14px */
      color: #6b7280; /* slateâ€‘500 */
    }

    /* â€”â€”â€” Responsive tweaks â€”â€”â€” */
    @media (min-width: 768px) {
      .hero h1 {
        font-size: 3.75rem; /* 60px */
      }
    }
  </style>
</head>
<body>
  <!-- ===== HERO ===== -->
  <header class="hero">
    <h1>Autoâ€‘Patching</h1>
    <p class="subtitle">Enhancing Multiâ€‘Hop Reasoning in Language Models</p>
  </header>

  <!-- ===== MAIN CARD ===== -->
  <main class="container">
    <article class="card">
      <div class="authors">AvivÂ Jan<sup>*</sup>, DanielÂ Tahory<sup>*</sup>, OrÂ Talmi<sup>*</sup>, OrÂ AboÂ Mokh<sup>*</sup></div>
      <div class="institution">Independent Undergraduate Research Project</div>

      <h2 class="section-title">Abstract</h2>
      <p class="abstract">
        Multiâ€‘hop questions still stump large language models (LLMs), which struggle to link information across multiple reasoning steps.
        We introduce <strong>Autoâ€‘Patch</strong>, a method that dynamically patches hidden states during inference to enhance multiâ€‘hop reasoning in LLMs.
        Building on the PatchScopes framework, Autoâ€‘Patch selectively modifies internal representations via a learned classifier.
        Evaluated on the <em>MuSiQue</em> dataset, Autoâ€‘Patch lifts the solve rate from 18.45&nbsp;% (baseline) to <strong>23.63&nbsp;Â±&nbsp;0.7&nbsp;%</strong> (3 runs), narrowing the gap to Chainâ€‘ofâ€‘Thought prompting (27.44&nbsp;%).
        Our results highlight the potential of dynamic hiddenâ€‘state interventions for advancing complex reasoning in LLMs.
      </p>

      <h2 class="section-title">Preprint</h2>
      <a href="AutoPatch.pdf" class="btn" target="_blank">ðŸ“„Â DownloadÂ PDF</a>

      <h2 class="section-title">Citation</h2>
      <div class="bibtex">
@misc{jan2025autopatching,
  title = {Autoâ€‘Patching: Enhancing Multiâ€‘Hop Reasoning in Language Models},
  author = {AvivÂ Jan and DanielÂ Tahory and OrÂ Talmi and OrÂ AboÂ Mokh},
  year = {2025},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL}
}
      </div>
    </article>
  </main>

  <!-- ===== FOOTER ===== -->
  <footer class="footer">
    Â©Â 2025Â AvivÂ JanÂ etÂ al. All rights reserved.
  </footer>
</body>
</html>
