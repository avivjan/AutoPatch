<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Autoâ€‘Patching: Enhancing Multiâ€‘Hop Reasoning in Language Models</title>

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;700;800&display=swap" rel="stylesheet" />
  <!-- Unicons for bullets -->
  <link rel="stylesheet" href="https://unpkg.com/@iconscout/unicons@4.0.8/css/line.css">
  <!-- Chart.js -->
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>

  <style>
    :root {
      --violet-600:#7c3aed;
      --blue-500:#3b82f6;
      --indigo-400:#6366f1;
      --white:#ffffff;
      --slate-50:#f8fafc;
      --slate-100:#f1f5f9;
      --slate-200:#e2e8f0;
      --slate-700:#374151;
      --slate-900:#111827;
    }
    *{margin:0;padding:0;box-sizing:border-box;}
    body{font-family:'Inter',Arial,sans-serif;background:var(--slate-50);color:var(--slate-900);line-height:1.6;scroll-behavior:smooth;}

    /* ===== HERO ===== */
    .hero{position:relative;padding:6rem 1.5rem 9rem;text-align:center;background:linear-gradient(135deg,var(--violet-600) 0%,var(--blue-500) 100%);color:var(--white);overflow:hidden;}
    .blob{position:absolute;border-radius:50%;filter:blur(120px);opacity:0.55;animation:float 24s ease-in-out infinite alternate;}
    .blob:nth-child(1){width:520px;height:520px;background:rgba(255,255,255,0.2);top:-220px;left:-190px;}
    .blob:nth-child(2){width:380px;height:380px;background:rgba(255,255,255,0.12);bottom:-140px;right:-170px;}
    @keyframes float{to{transform:translateY(-60px) translateX(40px) scale(1.05);}}
    .hero-title{font-size:clamp(2.5rem,5vw,4rem);font-weight:800;background:linear-gradient(90deg,#fff 0%,#dbeafe 40%,#f3e8ff 60%,#fff 100%);-webkit-background-clip:text;-webkit-text-fill-color:transparent;background-clip:text;}
    .hero-subtitle{font-size:1.25rem;font-weight:500;margin-top:0.8rem;opacity:0.9;}

    /* ===== NAVBAR ===== */
    .nav{position:fixed;top:0;left:0;width:100%;background:rgba(255,255,255,0.92);backdrop-filter:blur(10px);box-shadow:0 1px 6px rgba(0,0,0,0.05);display:flex;justify-content:center;gap:1rem;padding:0.55rem 1rem;z-index:50;}
    .nav a{position:relative;cursor:pointer;font-size:0.95rem;font-weight:500;color:var(--slate-700);text-decoration:none;padding:0.4rem 0.9rem;border-radius:0.55rem;transition:background 0.18s,color 0.18s;}
    .nav a:hover{background:var(--blue-500);color:var(--white);}  

    /* ===== LAYOUT ===== */
    .container{max-width:1000px;margin:-5.5rem auto 3rem;padding:0 1.5rem;}
    .card{background:rgba(255,255,255,0.6);backdrop-filter:blur(14px) saturate(180%);border-radius:1.25rem;border:1px solid rgba(255,255,255,0.3);box-shadow:0 20px 45px rgba(0,0,0,0.08);padding:3rem 2.5rem;}

    /* ===== COMMON ELEMENTS ===== */
    .section-title{font-size:1.55rem;font-weight:600;color:var(--blue-500);margin:2.75rem 0 1.25rem;scroll-margin-top:6rem;}
    .authors{font-size:1.125rem;font-weight:500;margin-bottom:0.3rem;color:var(--slate-700);} 
    .institution{font-style:italic;color:#475569;margin-bottom:2rem;}

    .abstract{text-align:justify;font-size:1.125rem;}
    .badges{display:flex;flex-wrap:wrap;gap:0.7rem;margin-top:1.2rem;}
    .badge{background:var(--indigo-400);color:var(--white);padding:0.35rem 0.9rem;border-radius:9999px;font-size:0.82rem;font-weight:500;white-space:nowrap;}

    .btn{display:inline-block;margin-top:1rem;background:var(--blue-500);color:var(--white);padding:0.85rem 1.75rem;border-radius:0.85rem;font-size:1.05rem;font-weight:500;text-decoration:none;transition:transform 0.18s ease, background 0.18s ease;}
    .btn:hover{transform:translateY(-3px);background:#2563eb;}

    .bullet-list{list-style:none;display:grid;gap:0.75rem;margin-top:0.6rem;}
    .bullet-list li{display:flex;align-items:flex-start;gap:0.65rem;font-size:1.05rem;}
    .bullet-list i{color:var(--blue-500);font-size:1.2rem;margin-top:0.15rem;}

    table{width:100%;border-collapse:collapse;margin-top:0.8rem;font-size:1.05rem;}
    th,td{border:1px solid var(--slate-200);padding:0.65rem;text-align:center;}
    th{background:var(--slate-100);font-weight:600;}

    .bibtex{background:var(--slate-100);border-radius:0.9rem;padding:1rem 1.25rem;font-family:"Courier New",monospace;font-size:0.9rem;white-space:pre-wrap;color:var(--slate-700);word-break:break-all;}

    .footer{text-align:center;font-size:0.85rem;color:#64748b;padding:3rem 1rem 4rem;}
  </style>
</head>
<body>
  <!-- NAVBAR -->
  <nav class="nav">
    <a href="#abstract">Abstract</a>
    <a href="#contributions">Contributions</a>
    <a href="#method">Method</a>
    <a href="#experiments">Experiments</a>
    <a href="#future">Future&nbsp;Work</a>
  </nav>

  <!-- Decorative blobs -->
  <div class="blob"></div>
  <div class="blob"></div>

  <!-- HERO -->
  <header class="hero">
    <h1 class="hero-title">Autoâ€‘Patching</h1>
    <p class="hero-subtitle">Enhancing Multiâ€‘Hop Reasoning in Language Models</p>
  </header>

  <!-- MAIN CARD -->
  <main class="container">
    <article class="card">
      <!-- META -->
      <div class="authors">Aviv Jan<sup>*</sup>, Daniel Tahory<sup>*</sup>, Or Talmi<sup>*</sup>, Or AboÂ Mokh<sup>*</sup></div>
      <div class="institution">TelÂ AvivÂ University</div>

      <!-- KPI badges -->
      <div class="badges">
        <span class="badge">+5.18Â pts on <em>MuSiQue</em></span>
        <span class="badge">23.63Â Â±Â 0.7Â % solve</span>
        <span class="badge">7BÂ LLaMAÂ 2</span>
        <span class="badge">CodeÂ &Â DataÂ Open</span>
      </div>

      <!-- ABSTRACT -->
      <h2 id="abstract" class="section-title">Abstract</h2>
      <p class="abstract">
        Multiâ€‘hop questions challenge large language models (LLMs) because they require stitching clues across several reasoning steps.
        <strong>Autoâ€‘Patch</strong> dynamically <em>patches</em> hiddenâ€‘state activations during inference. Guided by a lightweight SVM classifier, it decides <em>where</em> and <em>how</em> to transplant representations, eliminating the manual prompt decomposition required by PatchScopes.
        Evaluated on the <em>MuSiQue</em> benchmark, Autoâ€‘Patch lifts LLaMAâ€‘2â€‘7B from 18.45Â % to <strong>23.63Â Â±Â 0.7Â %</strong> solve rate, closing 57Â % of the gap to Chainâ€‘ofâ€‘Thought prompting while remaining zeroâ€‘shot and parameterâ€‘free.
      </p>

      <!-- LINKS -->
      <a href="https://arxiv.org/abs/2506.00483" target="_blank" class="btn">ðŸ“„ View onÂ arXiv</a>
      <a href="https://github.com/omertalmi5/Auto-Patching" target="_blank" class="btn" style="margin-left:0.6rem;background:var(--indigo-400);">ðŸ”— GitHub Repo</a>

      <!-- CONTRIBUTIONS -->
      <h2 id="contributions" class="section-title">Key Contributions</h2>
      <ul class="bullet-list">
        <li><i class="uil uil-wrench"></
